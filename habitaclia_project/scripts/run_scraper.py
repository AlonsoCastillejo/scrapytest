#!/usr/bin/env python3
"""
Script principal para ejecutar el scraper de Habitaclia
Uso: python scripts/run_scraper.py --cities barcelona madrid --type rent --pages 2
"""
import argparse
import sys
from pathlib import Path

# A√±adir src al path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from habitaclia.scraper.core import HabitacliaMultiCityScraper
from habitaclia.config.cities import SPANISH_CITIES, CITY_GROUPS
from habitaclia.data.validator import PropertyDataValidator

def parse_arguments():
    """Configura y parsea argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description='Habitaclia Real Estate Scraper',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  # Scrapear Barcelona y Madrid, alquiler, 2 p√°ginas
  python scripts/run_scraper.py --cities barcelona madrid --type rent --pages 2
  
  # Scrapear todas las ciudades principales
  python scripts/run_scraper.py --group tier1 --type homes --pages 1
  
  # Scrapear una ciudad espec√≠fica con m√°s p√°ginas
  python scripts/run_scraper.py --cities valencia --pages 5 --output valencia_rent.csv
        """
    )
    
    # Grupo principal de argumentos
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--cities', 
        nargs='+', 
        choices=list(SPANISH_CITIES.keys()),
        help='Lista de ciudades a scrapear'
    )
    group.add_argument(
        '--group',
        choices=['tier1', 'tier2', 'all_main'],
        help='Grupo predefinido de ciudades'
    )
    
    # Configuraci√≥n del scraping
    parser.add_argument(
        '--type', 
        choices=['rent', 'homes'], 
        default='rent',
        help='Tipo de propiedad (default: rent)'
    )
    parser.add_argument(
        '--pages', 
        type=int, 
        default=2,
        help='P√°ginas m√°ximas por ciudad (default: 2)'
    )
    parser.add_argument(
        '--output', 
        help='Nombre del archivo de salida (opcional)'
    )
    
    # Configuraci√≥n de delays
    parser.add_argument(
        '--fast',
        action='store_true',
        help='Modo r√°pido con delays reducidos (m√°s riesgo de detecci√≥n)'
    )
    parser.add_argument(
        '--slow',
        action='store_true',
        help='Modo lento con delays aumentados (m√°s seguro)'
    )
    
    # Opciones adicionales
    parser.add_argument(
        '--validate',
        action='store_true',
        help='Validar calidad de datos despu√©s del scraping'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='Limpiar datos autom√°ticamente (remover duplicados e inv√°lidos)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Mostrar informaci√≥n detallada'
    )
    
    return parser.parse_args()

def get_scraping_config(args):
    """Genera configuraci√≥n de scraping basada en argumentos"""
    config = {}
    
    # Configurar delays seg√∫n modo
    if args.fast:
        config.update({
            'delay_between_pages': (1, 3),
            'delay_between_properties': (1, 2),
            'delay_between_cities': (10, 20)
        })
        print("‚ö° Modo R√ÅPIDO activado - Mayor riesgo de detecci√≥n")
    elif args.slow:
        config.update({
            'delay_between_pages': (5, 10),
            'delay_between_properties': (3, 8),
            'delay_between_cities': (60, 120)
        })
        print("üêå Modo LENTO activado - M√°s seguro pero tardar√° m√°s")
    else:
        print("üöÄ Modo NORMAL activado - Balance entre velocidad y seguridad")
    
    return config

def show_cities_info(cities):
    """Muestra informaci√≥n sobre las ciudades a scrapear"""
    print("\nüèôÔ∏è  Ciudades seleccionadas:")
    print("=" * 50)
    total_estimated_time = 0
    
    for city in cities:
        city_name = SPANISH_CITIES.get(city, city)
        print(f"  üìç {city:<15} ‚Üí {city_name}")
        total_estimated_time += 15  # Estimaci√≥n: 15 min por ciudad
    
    print("=" * 50)
    print(f"‚è±Ô∏è  Tiempo estimado: {total_estimated_time} minutos")
    print(f"üìä Total ciudades: {len(cities)}")

def main():
    """Funci√≥n principal"""
    args = parse_arguments()
    
    # Determinar ciudades a scrapear
    if args.cities:
        cities = args.cities
    else:
        cities = CITY_GROUPS[args.group]
    
    # Mostrar informaci√≥n
    print("üè† HABITACLIA SCRAPER - FASE ALPHA")
    print("=" * 60)
    show_cities_info(cities)
    
    # Configuraci√≥n de scraping
    config = get_scraping_config(args)
    
    print(f"\nüîß Configuraci√≥n:")
    print(f"   Tipo: {args.type}")
    print(f"   P√°ginas por ciudad: {args.pages}")
    if args.output:
        print(f"   Archivo de salida: {args.output}")
    
    # Confirmaci√≥n del usuario
    if len(cities) > 3:
        response = input(f"\n‚ö†Ô∏è  Vas a scrapear {len(cities)} ciudades. ¬øContinuar? (y/N): ")
        if response.lower() != 'y':
            print("‚ùå Operaci√≥n cancelada")
            return
    
    print("\nüöÄ Iniciando scraping...")
    print("=" * 60)
    
    # Ejecutar scraping
    try:
        with HabitacliaMultiCityScraper(config) as scraper:
            # Scrapear datos
            properties_data = scraper.scrape_multiple_cities(
                cities=cities,
                property_type=args.type,
                max_pages=args.pages
            )
            
            if not properties_data:
                print("‚ùå No se obtuvieron datos")
                return
            
            print(f"\n‚úÖ Scraping completado: {len(properties_data)} propiedades")
            
            # Validaci√≥n de datos
            if args.validate:
                print("\nüîç Validando calidad de datos...")
                validator = PropertyDataValidator()
                validation_report = validator.validate_dataset(properties_data)
                
                print(f"üìä Reporte de validaci√≥n:")
                print(f"   Propiedades v√°lidas: {validation_report['valid_properties']}/{validation_report['total_properties']} ({validation_report['validity_rate']}%)")
                print(f"   URLs duplicadas: {validation_report['duplicate_urls']}")
                
                print("\nüí° Recomendaciones:")
                for rec in validation_report['recommendations']:
                    print(f"   {rec}")
                
                # Limpieza autom√°tica si se solicita
                if args.clean and validation_report['validity_rate'] < 90:
                    print("\nüßπ Limpiando datos...")
                    properties_data, cleaning_report = validator.clean_dataset(properties_data)
                    print(f"   Eliminados: {cleaning_report['duplicates_removed']} duplicados, {cleaning_report['invalid_removed']} inv√°lidos")
                    print(f"   Datos finales: {len(properties_data)} propiedades")
            
            # Guardar datos
            print("\nüíæ Guardando datos...")
            if args.output:
                csv_filename = args.output if args.output.endswith('.csv') else f"{args.output}.csv"
                json_filename = args.output.replace('.csv', '.json') if args.output.endswith('.csv') else f"{args.output}.json"
                scraper.save_data(csv_filename, json_filename)
            else:
                scraper.save_data()
            
            # Estad√≠sticas finales
            if args.verbose:
                print("\nüìà Estad√≠sticas finales:")
                stats = scraper.get_statistics()
                print(f"   Total propiedades: {stats['total_properties']}")
                print(f"   Ciudades scrapeadas: {stats['cities_scraped']}")
                
                if 'price_stats' in stats:
                    price_stats = stats['price_stats']
                    print(f"   Precio promedio: {price_stats['avg_price']:.2f}‚Ç¨")
                    print(f"   Rango de precios: {price_stats['min_price']:.0f}‚Ç¨ - {price_stats['max_price']:.0f}‚Ç¨")
                
                print("\nüèôÔ∏è  Propiedades por ciudad:")
                for city, count in stats['properties_by_city'].items():
                    print(f"     {city}: {count}")
            
            print("\nüéâ ¬°Proceso completado exitosamente!")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error durante el scraping: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
