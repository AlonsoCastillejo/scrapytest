#!/usr/bin/env python3
"""
Script principal para ejecutar el scraper de Habitaclia
Uso: python scripts/run_scraper.py --cities barcelona madrid --type rent --pages 2
"""
import argparse
import sys
from pathlib import Path

# A√±adir src al path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

try:
    from habitaclia.scraper.core import HabitacliaMultiCityScraper
    from habitaclia.config.cities import SPANISH_CITIES, CITY_GROUPS
except ImportError:
    # Fallback si la estructura modular no est√° completa
    print("‚ö†Ô∏è  Estructura modular no encontrada, usando import directo...")
    sys.path.insert(0, str(project_root))
    from src.habitaclia.scraper.core import HabitacliaMultiCityScraper
    
    # Definir ciudades directamente si no existe el m√≥dulo
    SPANISH_CITIES = {
        'barcelona': 'Barcelona', 'madrid': 'Madrid', 'valencia': 'Valencia',
        'sevilla': 'Sevilla', 'bilbao': 'Bilbao', 'malaga': 'M√°laga',
        'zaragoza': 'Zaragoza', 'murcia': 'Murcia', 'palma': 'Palma de Mallorca'
    }
    
    CITY_GROUPS = {
        'tier1': ['barcelona', 'madrid', 'valencia', 'sevilla'],
        'tier2': ['bilbao', 'malaga', 'zaragoza', 'murcia', 'palma'],
        'all_main': ['barcelona', 'madrid', 'valencia', 'sevilla', 'bilbao', 'malaga', 'zaragoza']
    }

def parse_arguments():
    """Configura y parsea argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description='Habitaclia Real Estate Scraper',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  # Scrapear Barcelona y Madrid, alquiler, 2 p√°ginas
  python scripts/run_scraper.py --cities barcelona madrid --type rent --pages 2
  
  # Scrapear todas las ciudades principales
  python scripts/run_scraper.py --group tier1 --type homes --pages 1
  
  # Scrapear una ciudad espec√≠fica con m√°s p√°ginas
  python scripts/run_scraper.py --cities valencia --pages 5 --output valencia_rent.csv
        """
    )
    
    # Grupo principal de argumentos
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--cities', 
        nargs='+', 
        choices=list(SPANISH_CITIES.keys()),
        help='Lista de ciudades a scrapear'
    )
    group.add_argument(
        '--group',
        choices=['tier1', 'tier2', 'all_main'],
        help='Grupo predefinido de ciudades'
    )
    
    # Configuraci√≥n del scraping
    parser.add_argument(
        '--type', 
        choices=['rent', 'homes'], 
        default='rent',
        help='Tipo de propiedad (default: rent)'
    )
    parser.add_argument(
        '--pages', 
        type=int, 
        default=2,
        help='P√°ginas m√°ximas por ciudad (default: 2)'
    )
    parser.add_argument(
        '--output', 
        help='Nombre del archivo de salida (opcional)'
    )
    
    # Configuraci√≥n de delays
    parser.add_argument(
        '--fast',
        action='store_true',
        help='Modo r√°pido con delays reducidos (m√°s riesgo de detecci√≥n)'
    )
    parser.add_argument(
        '--slow',
        action='store_true',
        help='Modo lento con delays aumentados (m√°s seguro)'
    )
    
    # Opciones adicionales
    parser.add_argument(
        '--validate',
        action='store_true',
        help='Validar calidad de datos despu√©s del scraping'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='Limpiar datos autom√°ticamente (remover duplicados e inv√°lidos)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Mostrar informaci√≥n detallada'
    )
    
    return parser.parse_args()

def get_scraping_config(args):
    """Genera configuraci√≥n de scraping basada en argumentos"""
    config = {}
    
    # Configurar delays seg√∫n modo
    if args.fast:
        config.update({
            'delay_between_pages': (1, 3),
            'delay_between_properties': (1, 2),
            'delay_between_cities': (10, 20)
        })
        print("‚ö° Modo R√ÅPIDO activado - Mayor riesgo de detecci√≥n")
    elif args.slow:
        config.update({
            'delay_between_pages': (5, 10),
            'delay_between_properties': (3, 8),
            'delay_between_cities': (60, 120)
        })
        print("üêå Modo LENTO activado - M√°s seguro pero tardar√° m√°s")
    else:
        print("üöÄ Modo NORMAL activado - Balance entre velocidad y seguridad")
    
    return config

def show_cities_info(cities):
    """Muestra informaci√≥n sobre las ciudades a scrapear"""
    print("\nüèôÔ∏è  Ciudades seleccionadas:")
    print("=" * 50)
    total_estimated_time = 0
    
    for city in cities:
        city_name = SPANISH_CITIES.get(city, city)
        print(f"  üìç {city:<15} ‚Üí {city_name}")
        total_estimated_time += 15  # Estimaci√≥n: 15 min por ciudad
    
    print("=" * 50)
    print(f"‚è±Ô∏è  Tiempo estimado: {total_estimated_time} minutos")
    print(f"üìä Total ciudades: {len(cities)}")

def main():
    """Funci√≥n principal"""
    args = parse_arguments()
    
    # Determinar ciudades a scrapear
    if args.cities:
        cities = args.cities
    else:
        cities = CITY_GROUPS[args.group]
    
    # Mostrar informaci√≥n
    print("üè† HABITACLIA SCRAPER - FASE ALPHA")
    print("=" * 60)
    show_cities_info(cities)
    
    # Configuraci√≥n de scraping
    config = get_scraping_config(args)
    
    print(f"\nüîß Configuraci√≥n:")
    print(f"   Tipo: {args.type}")
    print(f"   P√°ginas por ciudad: {args.pages}")
    if args.output:
        print(f"   Archivo de salida: {args.output}")
    
    # Confirmaci√≥n del usuario
    if len(cities) > 3:
        response = input(f"\n‚ö†Ô∏è  Vas a scrapear {len(cities)} ciudades. ¬øContinuar? (y/N): ")
        if response.lower() != 'y':
            print("‚ùå Operaci√≥n cancelada")
            return
    
    print("\nüöÄ Iniciando scraping...")
    print("=" * 60)
    
    # Ejecutar scraping
    try:
        # ‚úÖ Crear scraper con configuraci√≥n (sin 'with')
        scraper = HabitacliaMultiCityScraper(config)
        
        # Scrapear datos
        properties_data = scraper.scrape_multiple_cities(
            cities=cities,
            property_type=args.type,
            max_pages=args.pages
        )
        
        if not properties_data:
            print("‚ùå No se obtuvieron datos")
            return
        
        print(f"\n‚úÖ Scraping completado: {len(properties_data)} propiedades")
        
        # Validaci√≥n de datos (comentada hasta implementar)
        if args.validate:
            print("\nüîç Validando calidad de datos...")
            print("‚ö†Ô∏è  Validaci√≥n no implementada a√∫n - ser√° a√±adida en pr√≥xima versi√≥n")
            
            # TODO: Implementar cuando est√© disponible
            # try:
            #     from habitaclia.data.validator import PropertyDataValidator
            #     validator = PropertyDataValidator()
            #     validation_report = validator.validate_dataset(properties_data)
            #     print(f"üìä Propiedades v√°lidas: {validation_report['valid_properties']}")
            # except ImportError:
            #     print("‚ö†Ô∏è  M√≥dulo de validaci√≥n no disponible")
        
        # ‚úÖ Guardar datos usando m√©todos correctos
        print("\nüíæ Guardando datos...")
        if args.output:
            # Preparar nombres de archivo
            if args.output.endswith('.csv'):
                csv_filename = args.output
                json_filename = args.output.replace('.csv', '.json')
            else:
                csv_filename = f"{args.output}.csv"
                json_filename = f"{args.output}.json"
            
            # Guardar con nombres espec√≠ficos
            scraper.save_to_csv(csv_filename)
            scraper.save_to_json(json_filename)
            print(f"üìÅ Archivos guardados: {csv_filename}, {json_filename}")
        else:
            # Guardar con nombres autom√°ticos basados en timestamp
            scraper.save_to_csv()
            scraper.save_to_json()
        
        # Estad√≠sticas finales
        if args.verbose:
            print("\nüìà Estad√≠sticas finales:")
            stats = scraper.get_statistics()
            
            if 'error' not in stats:
                print(f"   Total propiedades: {stats['total_properties']}")
                print(f"   Ciudades scrapeadas: {stats['cities_scraped']}")
                
                # Estad√≠sticas de precios
                if 'price_stats' in stats:
                    price_stats = stats['price_stats']
                    print(f"   Precio promedio: {price_stats['avg_price']:.2f}‚Ç¨")
                    print(f"   Rango: {price_stats['min_price']:.0f}‚Ç¨ - {price_stats['max_price']:.0f}‚Ç¨")
                    print(f"   Propiedades con precio: {price_stats['properties_with_price']}")
                
                # Estad√≠sticas de √°rea
                if 'area_stats' in stats:
                    area_stats = stats['area_stats']
                    print(f"   √Årea promedio: {area_stats['avg_area']:.1f}m¬≤")
                    print(f"   Propiedades con √°rea: {area_stats['properties_with_area']}")
                
                print(f"\nüèôÔ∏è  Propiedades por ciudad:")
                for city, count in stats['properties_by_city'].items():
                    print(f"     {city}: {count}")
            else:
                print(f"   ‚ùå {stats['error']}")
        
        print("\nüéâ ¬°Proceso completado exitosamente!")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error durante el scraping: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()